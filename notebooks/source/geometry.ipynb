{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMC and its variant NUTS use gradient information to draw (approximate) samples from a posterior distribution. \n",
    "These gradients are computed in a particular coordinate system, and different choices of coordinate system can make HMC more or less efficient. \n",
    "For this reason it is important to pay attention to the *geometry* of the posterior distribution. \n",
    "Reparameterizing the model (i.e. changing the coordinate system) can make a big practical difference for many complex models. \n",
    "For the most complex models it can be absolutely essential. For the same reason it can be important to pay attention to some of the hyperparameters that control NUTS (in particular the `max_tree_depth`). \n",
    "\n",
    "In this tutorial we explore models with bad posterior geometries---and what one can do to get achieve better performance---with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.diagnostics import summary\n",
    "\n",
    "from numpyro.infer import MCMC, NUTS, init_to_uniform\n",
    "assert numpyro.__version__.startswith('0.7.2')\n",
    "\n",
    "# NB: replace gpu by gpu to run this notebook on gpu\n",
    "numpyro.set_platform(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by writing a helper function to do NUTS inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, \n",
    "                  num_warmup=1000, \n",
    "                  num_samples=1000,\n",
    "                  max_tree_depth=10, \n",
    "                  dense_mass=False, \n",
    "                  init_strategy=init_to_uniform):\n",
    "    kernel = NUTS(model, \n",
    "                  init_strategy=init_strategy, \n",
    "                  max_tree_depth=max_tree_depth,\n",
    "                  dense_mass=dense_mass)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=num_warmup,\n",
    "        num_samples=num_samples,\n",
    "        num_chains=1,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "    mcmc.run(random.PRNGKey(0))\n",
    "    summary_dict = summary(mcmc.get_samples(), 0.90, group_by_chain=False) \n",
    "    \n",
    "    # print the largest r_hat for each variable\n",
    "    for k, v in summary_dict.items():\n",
    "        spaces = \" \" * max(15 - len(k), 0)\n",
    "        print(\"[{}] {} \\t max r_hat: {:.4f}\".format(k, spaces, np.max(v['r_hat'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating HMC/NUTS\n",
    "\n",
    "In general it is difficult to assess whether the samples returned from HMC or NUTS represent accurate (approximate) posterior samples from the posterior. \n",
    "Two general rules of thumb, however, are to look at the effective sample size (ESS) and r_hat diagnostics returned by `mcmc.print_summary()`.\n",
    "If we see values of r_hat in the range `(1.0, 1.05)` and effective sample sizes that are comparable to the total number of samples `num_samples` (assuming `thinning=1`) then we have good reason to believe that HMC is doing a good job. \n",
    "If, however, we see low effective sample sizes or large r_hats for some of the variables (e.g. r_hat = 1.51) then HMC is likely struggling with the posterior geometry. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model reparameterization\n",
    "\n",
    "We begin with an example (horseshoe regression; see also examples/horseshoe.py) where reparameterization helps. \n",
    "This particular example demonstrates a general reparameterization strategy that is useful in many models with hierarchical/multi-level structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unreparameterized model (bad r_hats)\n",
      "[betas]            \t max r_hat: 1.1636\n",
      "[lambdas]          \t max r_hat: 1.0351\n",
      "[tau]              \t max r_hat: 1.0884\n",
      "\n",
      "reparameterized model (good r_hats)\n",
      "[betas]            \t max r_hat: 1.0108\n",
      "[lambdas]          \t max r_hat: 1.0132\n",
      "[tau]              \t max r_hat: 1.0003\n",
      "[unscaled_betas]   \t max r_hat: 1.0041\n"
     ]
    }
   ],
   "source": [
    "# In this unreparameterized model some of the parameters of the distributions\n",
    "# explicitly depend on other parameters (i.e. beta depends on lamba and tau).\n",
    "# This kind of coordinate system can be a challenge for HMC.\n",
    "def _unrep_hs_model(X, Y):\n",
    "    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n",
    "    betas = numpyro.sample(\"betas\", dist.Normal(tau * lambdas))\n",
    "    mean_function = jnp.dot(X, betas)\n",
    "    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)\n",
    "\n",
    "# In this reparameterized model none of the parameters of the distributions\n",
    "# explicitly depend on other parameters. \n",
    "# These two models are exactly equivalent but are expressed \n",
    "# in different coordinate systems.\n",
    "def _rep_hs_model(X, Y):\n",
    "    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n",
    "    unscaled_betas = numpyro.sample(\"unscaled_betas\", dist.Normal(jnp.ones(X.shape[1])))\n",
    "    scaled_betas = numpyro.deterministic(\"betas\", tau * lambdas * unscaled_betas)\n",
    "    mean_function = jnp.dot(X, scaled_betas)\n",
    "    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)    \n",
    " \n",
    "# create fake dataset\n",
    "X = np.random.RandomState(0).randn(100, 500)\n",
    "Y = X[:, 0]\n",
    "\n",
    "print(\"unreparameterized model (bad r_hats)\")\n",
    "run_inference(partial(_unrep_hs_model, X, Y))\n",
    "\n",
    "print(\"\\nreparameterized model (good r_hats)\")\n",
    "run_inference(partial(_rep_hs_model, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass matrices\n",
    "By default HMC/NUTS use diagonal mass matrices. \n",
    "For models with complex geometries it can pay to use a richer set of mass matrices.\n",
    "In this simple example we show that using a full-rank (i.e. dense) mass matrix leads to better r_hats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_mass = False (bad r_hats)\n",
      "[x]                \t max r_hat: 1.3810\n",
      "dense_mass = True (good r_hats)\n",
      "[x]                \t max r_hat: 0.9992\n"
     ]
    }
   ],
   "source": [
    "# because rho is almost 1.0 the posterior geometry is extremely skewed and using\n",
    "# the \"diagonal\" coordinate system implied by dense_mass=False leads to bad results\n",
    "rho = 0.9999\n",
    "true_cov = jnp.array([[10.0, rho], [rho, 0.1]])\n",
    "\n",
    "def mvn_model():\n",
    "    numpyro.sample(\"x\", \n",
    "        dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=true_cov)\n",
    "    )\n",
    "    \n",
    "print(\"dense_mass = False (bad r_hats)\")\n",
    "run_inference(mvn_model, dense_mass=False, max_tree_depth=3)\n",
    "\n",
    "print(\"dense_mass = True (good r_hats)\")\n",
    "run_inference(mvn_model, dense_mass=True, max_tree_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMC and its variant NUTS use gradient information to draw (approximate) samples from a posterior distribution. \n",
    "These gradients are computed in a particular coordinate system, and different choices of coordinate system can make HMC more or less efficient. \n",
    "For this reason it is important to pay attention to the *geometry* of the posterior distribution. \n",
    "Reparameterizing the model (i.e. changing the coordinate system) can make a big practical difference for many complex models. \n",
    "For the most complex models it can be absolutely essential. For the same reason it can be important to pay attention to some of the hyperparameters that control HMC/NUTS (in particular the `max_tree_depth` and `dense_mass`). \n",
    "\n",
    "In this tutorial we explore models with bad posterior geometries---and what one can do to get achieve better performance---with a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q numpyro@git+https://github.com/pyro-ppl/numpyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.diagnostics import summary\n",
    "\n",
    "from numpyro.infer import MCMC, NUTS, init_to_uniform\n",
    "assert numpyro.__version__.startswith('0.7.2')\n",
    "\n",
    "# NB: replace cpu by gpu to run this notebook on gpu\n",
    "numpyro.set_platform(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by writing a helper function to do NUTS inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, \n",
    "                  num_warmup=1000, \n",
    "                  num_samples=1000,\n",
    "                  max_tree_depth=10, \n",
    "                  dense_mass=False, \n",
    "                  init_strategy=init_to_uniform):\n",
    "    \n",
    "    kernel = NUTS(model, \n",
    "                  init_strategy=init_strategy, \n",
    "                  max_tree_depth=max_tree_depth,\n",
    "                  dense_mass=dense_mass)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=num_warmup,\n",
    "        num_samples=num_samples,\n",
    "        num_chains=1,\n",
    "        progress_bar=False,\n",
    "    )\n",
    "    mcmc.run(random.PRNGKey(0))\n",
    "    summary_dict = summary(mcmc.get_samples(), group_by_chain=False) \n",
    "    \n",
    "    # print the largest r_hat for each variable\n",
    "    for k, v in summary_dict.items():\n",
    "        spaces = \" \" * max(15 - len(k), 0)\n",
    "        print(\"[{}] {} \\t max r_hat: {:.4f}\".format(k, spaces, np.max(v['r_hat'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating HMC/NUTS\n",
    "\n",
    "In general it is difficult to assess whether the samples returned from HMC or NUTS represent accurate (approximate) samples from the posterior. \n",
    "Two general rules of thumb, however, are to look at the effective sample size (ESS) and r_hat diagnostics returned by `mcmc.print_summary()`.\n",
    "If we see values of r_hat in the range `(1.0, 1.05)` and effective sample sizes that are comparable to the total number of samples `num_samples` (assuming `thinning=1`) then we have good reason to believe that HMC is doing a good job. \n",
    "If, however, we see low effective sample sizes or large r_hats for some of the variables (e.g. r_hat = 1.15) then HMC is likely struggling with the posterior geometry. \n",
    "In the following we will use `r_hat` as our primary diagnostic metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model reparameterization\n",
    "\n",
    "### Example #1\n",
    "\n",
    "We begin with an example (horseshoe regression; see [examples/horseshoe.py](https://github.com/pyro-ppl/numpyro/blob/master/examples/horseshoe.py) for a fully example script) where reparameterization helps a lot. \n",
    "This particular example demonstrates a general reparameterization strategy that is useful in many models with hierarchical/multi-level structure. \n",
    "For more discussion of some of the issues that can arise in hierarhical see reference [1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this unreparameterized model some of the parameters of the distributions\n",
    "# explicitly depend on other parameters (i.e. beta depends on lamba and tau).\n",
    "# This kind of coordinate system can be a challenge for HMC.\n",
    "def _unrep_hs_model(X, Y):\n",
    "    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n",
    "    betas = numpyro.sample(\"betas\", dist.Normal(tau * lambdas))\n",
    "    mean_function = jnp.dot(X, betas)\n",
    "    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with the bad geometry that results form this coordinate system we change coordinates using the following re-write logic.\n",
    "Instead of \n",
    "\n",
    "$$ \\beta \\sim {\\rm Normal}(0, \\lambda \\tau) $$\n",
    "we write\n",
    "$$ \\beta^\\prime \\sim {\\rm Normal}(0, 1) $$\n",
    "and\n",
    "$$ \\beta \\equiv \\lambda \\tau \\beta^\\prime  $$\n",
    "\n",
    "where $\\beta$ is now defined *deterministically* in terms of $\\lambda$, $\\tau$,\n",
    "and $\\beta^\\prime$. In effect we've changed to a coordinate system where the different\n",
    "latent variables are less correlated with one another. \n",
    "In this new coordinate system we can expect HMC with a diagonal mass matrix to behave much better than it would in the original coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unreparameterized model (bad r_hats)\n",
      "[betas]            \t max r_hat: 1.1636\n",
      "[lambdas]          \t max r_hat: 1.0351\n",
      "[tau]              \t max r_hat: 1.0884\n",
      "\n",
      "reparameterized model (good r_hats)\n"
     ]
    }
   ],
   "source": [
    "# In this reparameterized model none of the parameters of the distributions\n",
    "# explicitly depend on other parameters. These two models are exactly equivalent \n",
    "# but are expressed in different coordinate systems.\n",
    "def _rep_hs_model(X, Y):\n",
    "    lambdas = numpyro.sample(\"lambdas\", dist.HalfCauchy(jnp.ones(X.shape[1])))\n",
    "    tau = numpyro.sample(\"tau\", dist.HalfCauchy(jnp.ones(1)))\n",
    "    unscaled_betas = numpyro.sample(\"unscaled_betas\", dist.Normal(jnp.ones(X.shape[1])))\n",
    "    scaled_betas = numpyro.deterministic(\"betas\", tau * lambdas * unscaled_betas)\n",
    "    mean_function = jnp.dot(X, scaled_betas)\n",
    "    numpyro.sample(\"Y\", dist.Normal(mean_function, 0.05), obs=Y)    \n",
    " \n",
    "# create fake dataset\n",
    "X = np.random.RandomState(0).randn(100, 500)\n",
    "Y = X[:, 0]\n",
    "\n",
    "print(\"unreparameterized model (bad r_hats)\")\n",
    "run_inference(partial(_unrep_hs_model, X, Y))\n",
    "\n",
    "print(\"\\nreparameterized model (good r_hats)\")\n",
    "run_inference(partial(_rep_hs_model, X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: numpyro.deterministic\n",
    "\n",
    "In `_rep_hs_model` above we used [`numpyro.deterministic`](http://num.pyro.ai/en/stable/primitives.html?highlight=deterministic#numpyro.primitives.deterministic) to define `scaled_betas`.\n",
    "We note that using this primitive is not strictly necessary; however, it has the consequence that `scaled_betas` will appear in the trace and will thus appear in the summary reported by `mcmc.print_summary()`. In other words we could also have written:\n",
    "\n",
    "```\n",
    "scaled_betas = tau * lambdas * unscaled_betas\n",
    "```\n",
    "\n",
    "without invoking the `deterministic` primitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass matrices\n",
    "By default HMC/NUTS use diagonal mass matrices. \n",
    "For models with complex geometries it can pay to use a richer set of mass matrices.\n",
    "\n",
    "\n",
    "### Example #2\n",
    "In this first simple example we show that using a full-rank (i.e. dense) mass matrix leads to a better r_hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because rho is very close to 1.0 the posterior geometry is extremely skewed and using\n",
    "# the \"diagonal\" coordinate system implied by dense_mass=False leads to bad results\n",
    "rho = 0.9999\n",
    "true_cov = jnp.array([[10.0, rho], [rho, 0.1]])\n",
    "\n",
    "def mvn_model():\n",
    "    numpyro.sample(\"x\", \n",
    "        dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=true_cov)\n",
    "    )\n",
    "    \n",
    "print(\"dense_mass = False (bad r_hat)\")\n",
    "run_inference(mvn_model, dense_mass=False, max_tree_depth=3)\n",
    "\n",
    "print(\"dense_mass = True (good r_hat)\")\n",
    "run_inference(mvn_model, dense_mass=True, max_tree_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example #3\n",
    "\n",
    "Using `dense_mass=True` can be very expensive when the dimension of the latent space `D` is very large. In addition it can be difficult to estimate a full-rank mass matrix with `D^2` parameters using a moderate number of samples if `D` is large. In these cases `dense_mass=True` can be a poor choice.  Luckily, the argument `dense_mass` can also be used to specify structured mass matrices that are richer than a diagonal mass matrix but more constrained (i.e. have fewer parameters) than a full-rank mass matrix ([see the docs](http://num.pyro.ai/en/stable/mcmc.html#hmc)).\n",
    "In this second example we show how we can use `dense_mass` to specify such a structured mass matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 0.9\n",
    "true_cov = jnp.array([[10.0, rho], [rho, 0.1]])\n",
    "\n",
    "# In this model x1 and x2 are highly correlated with one another\n",
    "# but not correlated at all with y.\n",
    "def partially_correlated_model():\n",
    "    x1 = numpyro.sample(\"x1\", \n",
    "        dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=true_cov)\n",
    "    )\n",
    "    x2 = numpyro.sample(\"x2\", \n",
    "        dist.MultivariateNormal(jnp.zeros(2), covariance_matrix=true_cov)\n",
    "    )    \n",
    "    y = numpyro.sample(\"y\", dist.Normal(jnp.zeros(100), 1.0))\n",
    "    numpyro.sample(\"obs\", dist.Normal(x1 - x2, 0.1), jnp.ones(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dense_mass = False (very bad r_hats)\")\n",
    "run_inference(partially_correlated_model, dense_mass=False, max_tree_depth=3)\n",
    "\n",
    "print(\"\\ndense_mass = True (bad r_hats)\")\n",
    "run_inference(partially_correlated_model, dense_mass=True, max_tree_depth=3)\n",
    "\n",
    "# We use dense_mass=[(\"x1\", \"x2\")] to specify\n",
    "# a structured mass matrix in which the y-part of the mass matrix is diagonal\n",
    "# and the (x1, x2) block of the mass matrix is full-rank.\n",
    "\n",
    "# Graphically:\n",
    "#      x1 x2 y\n",
    "#  x1 | * * 0 |\n",
    "#  x2 | * * 0 |\n",
    "#  y  | 0 0 * |\n",
    "\n",
    "print(\"\\nstructured mass matrix (good r_hats)\")\n",
    "run_inference(partially_correlated_model, dense_mass=[(\"x1\", \"x2\")], max_tree_depth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other strategies\n",
    "\n",
    "- In some cases it can make sense to use variational inference to *learn* a new coordinate system. For details see [examples/neutra.py](https://github.com/pyro-ppl/numpyro/blob/master/examples/neutra.py) and reference [3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] \"Hamiltonian Monte Carlo for Hierarchical Models,\"\n",
    "    M. J. Betancourt, Mark Girolami.\n",
    "\n",
    "[2] \"Slice sampling,\" R. M. Neal.\n",
    "\n",
    "[3] \"NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport,\"\n",
    "    Matthew Hoffman, Pavel Sountsov, Joshua V. Dillon, Ian Langmore, Dustin Tran, Srinivas Vasudevan.\n",
    "    \n",
    "[4] \"Reparameterization\" in the Stan user's manual.\n",
    "    https://mc-stan.org/docs/2_27/stan-users-guide/reparameterization-section.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
